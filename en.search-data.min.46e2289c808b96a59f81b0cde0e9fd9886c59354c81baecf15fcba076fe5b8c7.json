[{"id":0,"href":"/docs/spring24/00_taco_example/","title":"00 Taco Example","section":"Spring24","content":" Example : Content # This paper propose a compression framework that leverages text information mainly by text-adaptive encoding and training with joint image-text loss. By doing so, they avoid decoding based on text-guided generative models\u0026mdash;known for high generative diversity\u0026mdash;and effectively utilize the semantic information of text at a global level.\nExample : Using KaTeX for math equation # KaTeX shortcode let you render math typesetting in markdown document. See KaTeX\nHere is some inline example: \\(\\pi(x)\\) , rendered in the same line. And below is display example, having display: block \\[f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi\\] Text continues here!!!\n"},{"id":1,"href":"/docs/spring24/01_/","title":"01","section":"Spring24","content":" # "},{"id":2,"href":"/docs/spring24/02_/","title":"02","section":"Spring24","content":" Spectrally Pruned Gaussian Fields with Neural Compensation (SUNDAE) # *Team: Donggeon Lee, Chiho yoon *Authors: Yang, Runyi, et al\nSummary # Fig1. Comparison of 3D gaussian splatting and proposed SUNDAE Conventional 3D Gaussian Splatting (3DGS) # Pros: Superior rendering speed and quality Cons: High memory consumption Proposed SUNDAE # It constructs a memory-efficient Gaussian field using spectral pruning and neural compensation. It considers the relationship between primitives, reducing memory usage while maintaining rendering quality. It significantly reduces memory consumption while preserving high rendering quality. Code: https://runyiyang.github.io/projects/SUNDAE/. Introduction # Fig2. Conceptual illustration of vanilla 3DGS, SUNDAE spectral pruning technique, and neural compensation. 3DGS # Recently, 3DGS has been proposed as a novel 3D scene representation, utilizing a set of 3D positions, opacity, anisotropic covariance, and spherical harmonic (SH) coefficients to represent a 3D scene (left panel of Fig. 2). 3DGS demonstrates notable advantages in rendering speed, rendering quality, and training time. But it requires a large storage.\nSpectral graph pruning # Gaussian fields utilize a collection of Gaussian primitives as the representation of the scene. As these primitives are irregularly distributed in 3D space, we propose a graph-based data structure, rather than regular structures like grids, to capture the relationship between these primitives (middle panel of Fig. 2).\nNeural compensation # To address an inevitable decrease in rendering quality, they employ a neural compensation head to compensate for this quality loss (right panel of Fig. 2).\nContributions # A newly proposed primitive pruning framework for Gaussian fields based upon the spectrum of primitive graphs; A novel feature splatting and mixing module to compensate for the performance drop caused by the pruning; State-of-the-art results, in terms of both quality and speed, on various benchmarks with low memory footprint. Methods # Results # Quantitative Results # Table 1. Quatitative evaluation of SUNDAE. SUNDAE demonstrates strong performance across various metrics, including PSNR, SSIM, FPS, and memory usage.\nCompared to existing methods on the MipNeRF360 dataset, SUNDAE achieves a balance between rendering quality and efficiency, maintaining high FPS rates while significantly reducing memory consumption. Even at low sampling rates, SUNDAE remains competitive with established approaches, showcasing the effectiveness of its spectral pruning and neural compensation techniques in managing Gaussian primitive relationships and retaining scene information. Overall, SUNDAE represents scenes more compactly while maintaining high quality rendering. Qualitative Results # Fig3. Qualitative results of SUNDAE. The qualitative results demonstrate that SUNDAE achieves comparable novel view synthesis quality with significantly lower memory consumption (1% or 10%).\nThe graph effectively captures primitive relationships, while the neural compensation head preserves rendering quality. Spectral pruning notably removes outliers near the camera, enhancing scene coherence. Ablation Study # Fig4. Ablations experiment on the ratio ùõæ of the bandlimited filter of graph based pruning. Band-limited ratio of Graph-based pruning: The band-limited filter\u0026rsquo;s ratio, represented by ùõæ, significantly impacts rendering quality, with a ùõæ value of 50% yielding the most favorable outcomes, emphasizing the advantage of spectral pruning in preserving important high-frequency details and low-frequency background (Fig. 4). Table 2. Ablations of neural compensation module size. Fig 5. Visualization with and without neural compensation. The compensation performance of the network: Employing the neural compensation module enhances performance across all sampling rates evide(Table 2, Fig. 5), highlighting its compensatory capability in mitigating performance drops caused by spectral pruning and effectively modeling the relationship between primitives. Neural Compensation Module Size: Increasing the size of the neural compensation module does not necessarily enhance rendering quality (Table 2), aligning with findings from ADOP and indicating a balance between quality and memory usage. Conclusion # They propose SUNDAE, a novel approach to spectrally prune Gaussian fields with neural compensation, efficiently capturing the relationship between Gaussian primitives using graph signal processing and blending information to offset pruning-induced information loss. By leveraging spatial information among Gaussian primitives to construct a graph and spectrally pruning less significant ones, they employ a lightweight neural network to compensate for quality degradation post-pruning. Experimental findings demonstrate SUNDAE\u0026rsquo;s ability to maintain the efficiency of 3DGS while significantly reducing its size across various scenarios. "},{"id":3,"href":"/docs/spring24/03_/","title":"03","section":"Spring24","content":" Unit Scaling # Unit scaling is proposed to address the limitations of existing methods for managing scale in typical models. A model is considered unit-scaled if its activations, weights, and gradients have approximately unit variance at initialization. This is achieved by inserting scaling factors into the forward and backward passes. Unlike loss scaling, which requires an empirically determined hyperparameter or an adaptive algorithm, unit scaling determines these scales based on a set of rules for each operation, approximately preserving the variance of the inputs. This leads to global unit scaling throughout the model, ensuring tensor values are centered within the exponent range at initialization, providing headroom during training to avoid going out of range.\nA framework for scaling computational graphs # Computational Graphs\nRepresent model by the differentiable function \\(f_{model}(x_1,...,x_m)\\) Describe the structure of such a model using a directed acyclic graph (DAG) denoted \\(\\mathcal{G} =(\\mathcal{V}, \\mathcal{E}) \\) This kind of graph is commonly known as a computational graph, with vertices as nodes and their corresponding functions as ops. Forward and backward graphs\nWe refer to the computational graph corresponding to \\(f_{model}\\) as the forward graph In deep learning we typically apply reverse-mode automatic differentiation to the forward graph to create a second computational graph whose output nodes represent the partial derivatives of the model with respect to its inputs: \\( \\frac{\\partial f_{model}}{\\partial x_i}, \\forall i \\in[1 . . m] \\) . We call this the backward graph Scaled ops\nGiven an op \\(f\\left(x_1, \\ldots, x_k\\right)\\) , we define the scaled op \\( f^*\\left(x_1, \\ldots, x_k, \\alpha, \\beta_1, \\ldots, \\beta_k\\right) \\) with scaling factors \\( \\alpha, \\beta_1, \\ldots, \\beta_k \\in \\mathbb{R}^{\u0026#43;} \\) , such that \\(f^* \u0026amp; \\triangleq \\alpha \\cdot f\\left(x_1, \\ldots, x_k\\right)\\) \\( f_{\\text {grad }}^*\\left(x_1, \\ldots x_k, g\\right)_i \u0026amp; \\triangleq \\beta_i \\cdot f_{\\text {grad }}\\left(x_1, \\ldots x_k, g\\right)_i, \\forall i \\in[1 . . k] \\) Scaled computational graph\nA scaled computational graph is one where every op \\(f\\) in the forward graph is replaced by a scaled equivalent \\(f^{*}\\) , with the backward graph then generated to produce \\(f^{*}_{grad}\\) grad for each \\(f_{grad}\\) , using any choice of scaling factors. Constraint-scaled computational graphs\nA constraint-scaled computational graph is a scaled computational graph where we restrict the scaling factors of ops that consume non-cut-edge variables in the following way: for any edge \\(e \\notin \\mathcal{C}\\) , we require the op consuming the variable \\(x_e\\) to have scaling factors \\(\\alpha = \\beta_e f\\) . Proposition 5.1\nFor any scaled op, there is an equivalent unscaled op with the same training dynamics under a firstorder optimiser.\nTheorem 5.2\nA constraint-scaled computational graph itself represents a scaled op.\nA scaling strategy for unit variance # Unit scaled computational graphs\nInitially set aside any scale constraints, and calculate the scaling factors that give each op expected unit variance outputs (this process is covered below). Now resolve any scale constraints by taking each constrained group \\( {\\alpha, \\beta_1, \\ldots, \\beta_l } \\) and selecting the geometric mean \\( \\left(\\alpha, \\beta_1, \\ldots, \\beta_l \\right)^\\frac{1}{l\u0026#43;1} \\) Selecting scaling factors\nAssuming unit-scaled inputs to \\( y = f(x_i,\\ldots,x_k) \\) , derive the output scale \\( \\sigma_Y \\) and set the forward scaling factor \\( \\alpha = 1/\\sigma_Y \\) . Repeat this process for \\( x_i\u0026#39;=f_{grad}(\\ldots)_i, \\forall i \\in[1 . . k] \\) , to obtain the gradient scale \\( \\sigma_{x_i\u0026#39;} \\) i and set the backward scaling factor \\( \\beta_i = 1/\\sigma_{x_i\u0026#39;} \\) . Weighted addition # When tensors of different scales, such as those in residual layers, losses, and positional encodings, are added, simply adding them can adversely affect performance. To address this, we propose using weighted_add. In this approach, we can maintain unit scale while performing operations using a scaled identity function.\nRecipe # We now outline a high-level recipe for a unit-scaled model:\nInitialise non-bias parameters with unit variance. Calculate scaling factors for all scaled ops. Identify non-cut-edges, and constrain the ops consumingthem to have \\( \\alpha = \\beta \\) by taking the geometric mean. Replace adds with weighted adds. Example # Using the unit scaling recipe, we first build a scaled op, and then a full scaled layer. Consider a scaled projection op with learnable weights:\n\u0026laquo;\u0026laquo;\u0026laquo;\u0026lt; HEAD\n\\( \\operatorname{matmul}^*(X, W) \u0026amp; =\\alpha \\cdot X W \\) \\( \\operatorname{matmul}_{\\text {grad }}^*(X, W, G)_1 \u0026amp; =\\beta_1 \\cdot G W^{\\top} \\) \\( \\operatorname{matmul}_{\\text {grad }}^*(X, W, G)_2 \u0026amp; =\\beta_2 \\cdot X^{\\top} G \\) ======= \\( \\operatorname{matmul}^*(X, W) \u0026amp; =\\alpha \\cdot X W \\) \\( \\operatorname{matmul}_{\\text {grad }}^*(X, W, G)_1 \u0026amp; =\\beta_1 \\cdot G W^{\\top} \\) \\( \\operatorname{matmul}_{\\text {grad }}^*(X, W, G)_2 \u0026amp; =\\beta_2 \\cdot X^{\\top} G \\) 09d0440407f9953465e8c825cbdec2d13f3731c9 for input \\( X \\in \\mathbb{R}^{b \\times m} \\) , weight \\( W \\in \\mathbb{R}^{m \\times n} \\) , output \\( \\mathbb{R}^{b \\times n} \\) and incoming gradients \\( G \\in \\mathbb{R}^{b \\times n} \\) We show code for the above in Figure 3, which also gives a scaled layer for the Transformer FFN\nFig3. PyTorch examples Results # Character language modelling\nExperimental Setup: Train causal language models on WikiText-103 raw character language modeling, using cross-entropy loss during training and evaluating on bits per character (BPC). Below the product of these settings, we compare the performance of regular (baseline) and unit scaling in both FP32 and FP16.\nSequence layer type: Attention, RNN and Convolution Norm placement: PreNorm, PostNorm and NoNorm Residual scaling: default, fixed and running-mean Results\nFirst, these demonstrate the need for scaling when using FP16. This is due to gradient underflow, since loss scaling with a factor of 2048 resolves the issue. Second, they demonstrate that unit scaling, despite changing the training behaviour of the model beyond just numerics, matches or even slightly improves upon baseline performance in almost all cases. Finally, they show that no tuning is necessary when switching unit scaling to FP16. suggest that running-mean or fixed are reasonable choices when using unit scaling Fig4. Character language modelling, showing validation bits per character over a wide range of models Masked language modelling\nExperimental Setup\nResults\nTable2. Downstream performance of regular and unit-scaled BERT models Related Work # Variance scaling analysis\nFP8 inference\nDiscussion # "},{"id":4,"href":"/docs/spring24/04_/","title":"04","section":"Spring24","content":" # "},{"id":5,"href":"/docs/spring24/05_/","title":"05","section":"Spring24","content":" # "},{"id":6,"href":"/docs/spring24/06_/","title":"06","section":"Spring24","content":" # "},{"id":7,"href":"/docs/spring24/07_/","title":"07","section":"Spring24","content":" # "},{"id":8,"href":"/docs/spring24/08_/","title":"08","section":"Spring24","content":" # "},{"id":9,"href":"/docs/spring24/09_/","title":"09","section":"Spring24","content":" # "},{"id":10,"href":"/docs/spring24/10_/","title":"10","section":"Spring24","content":" # "},{"id":11,"href":"/docs/spring24/11_/","title":"11","section":"Spring24","content":" # "},{"id":12,"href":"/docs/spring24/12_/","title":"12","section":"Spring24","content":" # "},{"id":13,"href":"/docs/spring24/13_/","title":"13","section":"Spring24","content":" # "},{"id":14,"href":"/docs/spring24/14_/","title":"14","section":"Spring24","content":" # "},{"id":15,"href":"/docs/spring24/15_/","title":"15","section":"Spring24","content":" # "},{"id":16,"href":"/docs/spring24/16_/","title":"16","section":"Spring24","content":" Mixture-of-Depths: Dynamically allocating compute in transformer-based language models # Authors: Dativd Raposo(Google DeepMind) and Adam Santoro(Google DeepMind) et.al\n‚ÄúChoice and concentration‚Äù is an effective strategies for completing tasks with overall success. It is not necessary to consume same amount of effort and time into all problems. If we expend our energy on trivial issues, we may fail to concentrate on what truly matters. Similary, a technique was introduced that allows languge models to allocate less budget to non-essential tokens instead of focusing equally on all tokens.\nThe tremendous technique developed by Google DeepMind Researchers is called Miture-of-Depths, or MoD for short. In this blog post, we take a look at building blocks of MoD and how they works.\nThis paper insists that all problems do not require same amount of time to solve in real world and also in language models like Transformer.\nBut, Transformer models spread FLOPs \u0026lsquo;uniformly\u0026rsquo; across input sequences, which is inefficient!\nTherefore there are many efforts like \u0026ldquo;early exiting\u0026rdquo; to reduce total FLOPs \u0026amp; \u0026lsquo;dynamically\u0026rsquo; allocate compute budgets.\nHowever, these methods do not work well due to hardware constraints.\nSo, the methods should be sophistically addressed like harmonious with current hardware stack \u0026amp; known tensor sizes that are selected to maximize hardware utilization.\n(Fig 1. Description of overall MoD \u0026 comparison between Vanilla Transformer \u0026 Early Exit method) Therefore, the things that authors of this paper contributed in this paper are listed as follows:\nSuggestion of method(Mixture-of-Depths, MoD) which limits the total FLOPs by choosing only k tokens which process into Attention + mlp layer. Comparing this method with vanilla Transformer(isoFLOP) \u0026amp; Mixture-of-Experts(MoE) \u0026amp; Combined version of MoE + MoD = MoDE With this method, MoD achieves better performance than vanilla Transformer in isoFLOPs \u0026amp; faster inference speed. Background # Early Exit method # (Fig 2. Early Exit method) Early Exit method is a method when the model decides to end computation on a given token, then model skips the remaining layers.\nDifference between MoD is, MoD can choose whether skip middle layer or not, but Early Exit method can\u0026rsquo;t.\nWhat is MoE? # (Fig 3. Diagram of MoE) MoE is an model which consists of parallel expert models which is fitted to certain domains.\nLike MoD, token-level routing decisions are made across the network depth.\nDifference between MoD is, MoD chooses path to transformer or to residual connection, MoE chooses path to transformer(Expert) or to transformer(Expert) or both.\nImplementing Mixture-of-Depth Transformers # High-level strategy of Mixture-of-Depths is as follows:\nDefining a compute budget # First, to make smaller compute budget per forward pass than vanila transformer, we limit the number of tokens in a sequence for computations like self-attention and MLP. This concept, called capacity, defines the total tokens processed and determines the FLOPs required. For example, in vanila transformers, capacity($T$) covers all tokens, but in MoE transformers, it\u0026rsquo;s dividing among multiple experts. Lowering computation capacity can reduce the compute budget per forward pass without performance loss if the model learns to prioritize important tokens. Routing around transformer blocks # Routing schemes # Routing implementation # Sampling # Results \u0026amp; Discussion # References # Fig 2. https://www.sciencedirect.com/science/article/pii/S0893608022002532\nFig 3. https://deepgram.com/learn/mixture-of-experts-ml-model-guide\n"},{"id":17,"href":"/docs/spring24/18_/","title":"18","section":"Spring24","content":" # "},{"id":18,"href":"/docs/spring24/19_/","title":"19","section":"Spring24","content":" # "},{"id":19,"href":"/docs/spring24/20_/","title":"20","section":"Spring24","content":" # "},{"id":20,"href":"/docs/spring24/21_/","title":"21","section":"Spring24","content":" # "},{"id":21,"href":"/docs/spring24/22_/","title":"22","section":"Spring24","content":" Larimar: Large Language Models with Episodic Memory Control # Posted by: Sunggyu Jang, Hyeonwoo Park\nAuthors: Payel Das (IBM AI Research), Subhajit Chaudhury (IBM AI Research) et.al\n1. Background # Large Language Model (LLM) is one of the most famous topics in these days, due to their outstanding performance on various Natural Language Processing (NLP) tasks. However, LLM has faced a lot of challenges at the same time. In this report, we especially focus on the \u0026ldquo;knowledge edit\u0026rdquo; problem.\nKnowledge edit in LLM research # Knowledge edit problem can be summarized as \u0026ldquo;constantly updating the knowledge of pre-trained LLMs to keep models fact-relevant, safe, and ethical after deployment.\u0026rdquo; To be specific, model editing is mandatory to remove the undesired, incorrect, or obsolete facts from the LLM\u0026rsquo;s \u0026ldquo;memory\u0026rdquo;, and optionally replace it with desired outcome. Figures below illustrate why do we need knowledge update.\n(Fig1. Knowledge update: New knowledge should be injected constantly) (Fig2. Context length generalization: The ability to quickly update the LLM can help with \"input context length generalization problem\") (Fig3. Selective fact forgetting: LLMs should forget personal \u0026 sensitive data) Autoencoder # TODO : This content can be erased\n(Fig4. Autoencoder) Memory network # Neocortex-Hippocampus interactions # This paper imitates the role of brain. Humans can rapidly update their knowledge after encountering the first relevant instance. In the brain, this process is facilitated through interactions between the neocortex and the hippocampus. The hippocampus is the site for storing long-term memories, while the neocortex integrates long-term and short-term memories to relay the results to the body.\n(Fig6. Neocortex and the Hippocampus) The Complementary Learning Systems (CLS) theory proposes a model that combines these complementary learning systems of the hippocampus and neocortex. The interaction between the neocortex and hippocampus in the brain is known to promote adaptive behavior through memorization and generalization. Furthermore, it is suggested that memory consolidation from the hippocampus to the neocortex is facilitated by the activation synchronized with multiple exact or false replays of the encoded experience in the hippocampus. This implies that the hippocampus functions as a generative associative network. 2. Contributions # Larimar introduces a class of memory-conditioned language models inspired by complementary learning mechanisms in the brain. This architecture facilitates real-time test-time adaptation without requiring time-intensive gradient-based learning or internal fact tracing, offering a faster method for updating LLMs. Utility Demonstration in Knowledge Editing and Context Generalization:\nThe proposed method is demonstrated on two significant and challenging use cases: knowledge editing and generalizing to longer input contexts. Larimar exhibits fast and accurate training-free adaptation to new inputs in both scenarios, outperforming baseline editing methods and existing language models. Selective Fact Forgetting and Information Leakage Prevention:\nLarimar effectively supports selective fact forgetting and prevents information leakage using its one-shot memory updating mechanism. Recursive Search-Based Solution for Long Context Generalization: A simple recursive search-based approach is provided to enable Larimar\u0026rsquo;s memory to generalize to longer input contexts.\n3. Model architecture # Larimar consists of three main components: encoder, decoder, and adaptive memory.\nEncoder: Transforms the input into a latent vector Decoder: Generates an answer to the question conditioned on the memory Memory: Stores episodes in encoded form (Fig7. Larimar architecture) 4. Memory Operations # 5. Results # Wall Clock time # (Fig8. Wall Clock Time result) Single Fact Editing # (Fig9. ) Sequential Fact Editing # (Fig10. Selective fact forgetting: LLMs should forget personal \u0026 sensitive data) Selective Forgetting # (Fig11. Selective fact forgetting: LLMs should forget personal \u0026 sensitive data) Recall Performance # (Fig12. Selective fact forgetting: LLMs should forget personal \u0026 sensitive data) 6. Conclusion # 7. References # https://arxiv.org/abs/2310.16218 -\u0026gt; Knowledge Editing for Large Language Models: A Survey\nhttps://arxiv.org/abs/2207.04901 -\u0026gt; Exploring Length Generalization in Large Language Models\nhttps://arxiv.org/abs/2402.05813 -\u0026gt; Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models\nhttps://arxiv.org/abs/2403.11901\nbrain figure\nhttps://openreview.net/forum?id=Harn4_EZBw -\u0026gt; Generative Pseudo-Inverse Memory\n"},{"id":22,"href":"/docs/spring24/23_/","title":"23","section":"Spring24","content":" Beyond Language Models: Byte Models are Digital World Simulators # Authors: Shangda Wu, Xu Tan, Zili Wang, Rui Wang, Xiaobing Li, Maosong Sun\nByte models expand traditional language models to the byte level, starting from the premise that all digital data and operations are fundamentally byte-based. These models process data from various modalities such as text, audio, and images uniformly as bytes, increasing their applicability in a wide digital environment.\nIn this paper, bGPT is introduced. bGPT is designed to model digital data at the byte level and is optimized to effectively process byte sequences. It has demonstrated performance comparable to specialized models across various modalities, including text, audio, and images, and offers new possibilities for predicting, simulating, and diagnosing hardware operations. Designed to predict and understand bytes, bGPT provides a deeper understanding of and interaction with digital systems.\nThe bGPT framework simulates digital systems using native binary data. It integrates diverse data types into a single model by treating everything as a byte sequence. Exploring bGPT # Architecture\nLearning patterns in digital systems at the byte level provides a unified approach to integrating various data types, but the high resolution of bytes results in long sequences that significantly increase computational costs. This issue is especially pronounced in transformer-based models, limiting the efficiency and scalability of processing binary data. bGPT is equipped with a hierarchical structure designed to efficiently handle entire byte sequences. This structure segments a sequence of byte \\(B = \\{b_1, b_2, \\ldots, b_T\\}\\) of length \\(T\\) into a sequence of patches \\(\\mathcal{P}\\) , where each patch contains exactly \\(S\\) bytes: \\(\\mathcal{P} = [P_1, P_2, \\ldots, P_N]\\) where \\(N = \\left\\lceil \\frac{T}{S} \\right\\rceil\\) is the number of patches,\n\\(P_i = [b_{(i-1)S\u0026#43;1}, \\ldots, b_{(i)S}]\\) for \\(( 1 \\leq i \\leq N)\\) , if \\(T \\mod S \\neq 0\\) , the last patch defined as \\(P_N = [b_{(N-1)S\u0026#43;1}, \\ldots, b_T, \\underbrace{e, \\ldots, e}_{S - (T \\mod S)}]\\) where \\(e\\) represents the \u0026lt;eop\u0026gt; (end-of-patch).\nComponents\nLinear Projection Layer: Each byte patch is mapped to a high-dimensional feature space through a linear projection layer. During this process, each byte is encoded into a 257-dimensional vector, which includes the 256 possible byte values and a special \u0026lt;eop\u0026gt; (end-of-patch) token. Patch-Level Decoder: The embedded patches are processed by a patch-level decoder. This decoder plays a role in predicting the features of the next patch from the embedding of each patch, thereby learning the structural patterns of the entire dataset. Byte-Level Decoder: Based on the predicted patch features, the byte sequence within each patch is reconstructed. The byte-level decoder uses the features of each patch to predict the next byte within that patch, processing the detailed information of the entire byte sequence. Model Training\n1. Generative Modeling\nThis approach requires the model to predict the next byte in a given byte sequence. The model takes the byte sequence \\(B = \\{b_1, b_2, \\ldots, b_T\\}\\) as input and utilizes all previous byte information to predict the next byte \\(b_{i\u0026#43;1}\\) at each position.\nAs a loss function, the negative log likelihood of the next byte at each step is minimized. This encourages the model to maximize the likelihood of the actual occurrence of the next byte. \\(\\mathcal{L}_{\\text{GEN}}(\\theta) = - \\sum_{i=1}^{T-1} \\log p(b_{i\u0026#43;1} \\mid b_1, b_2, \\ldots, b_i; \\theta)\\) 2. Classification\nBased on the knowledge acquired through generative modeling, bGPT can also be applied to classification tasks for labeled datasets. In this process, the model takes a byte sequence as input and predicts the category to which that sequence belongs. For classification tasks, the loss function used is the cross-entropy loss, which ensures that the model accurately outputs the prediction probabilities for each category.\n\\(\\mathcal{L}_{\\text{CLF}}(\\theta) = - \\sum_{k=1}^{K} y_k \\log p(y_k \\mid B; \\theta)\\) These training objectives enable bGPT to understand various byte-based data and accurately mimic digital patterns of the real world. The combination of generative approaches and classification capabilities grants the model the flexibility to tackle a diverse range of problems. Through this, the model can go beyond simple pattern recognition to play a crucial role in predicting and analyzing the operations of complex digital systems.\nApplications # 1. Digital Media Processing\nbGPT is used for processing various types of digital media data such as text, audio, and images. This model performs learning targeted at media files through generative modeling, transforming the data into features and subsequently performing classification tasks based on these features.\nFor example, audio files are converted to and processed in WAV format, while images are processed at a low resolution in BMP format. By utilizing these standardized datasets, bGPT can develop a generalized understanding of various media types.\n2. Algorithm and Hardware Simulation\nbGPT is particularly useful for tasks such as data conversion and CPU state modeling. This means bGPT can learn the digital conversion process and simulate the operation of CPUs to predict the state of the CPU after various commands are executed.\nFor example, in the task of converting the music data format from ABC notation to MIDI format, bGPT learns to transform text-based music scores in ABC notation into binary performance signals in MIDI. Additionally, this model is also capable of performing the reverse conversion from MIDI back to ABC notation.\nExperiment # 1. Processing Various Digital Media\nExperiment Overview To assess the flexibility and versatility of the bGPT model, experiments with various types of digital media data were conducted. This involved handling a wide range of file types including text, audio, and image data, with the aim to measure the model\u0026rsquo;s ability to process these types and to see how well bGPT generalizes compared to specialized models. The experiment included both generative modeling and classification tasks.\nExperimental Data The datasets used in the experiment included:\nText: Wikipedia data and AG news dataset. Audio: LibriSpeech and Speech Commands v2 dataset. Images: ImageNet and CIFAR-10 dataset. These datasets are ideal resources for evaluating the diverse media processing capabilities of bGPT.\nExperimental Setup The bGPT model was trained under various settings:\nbGPTimage: Trained exclusively with image data (ImageNet). bGPTlibri: Trained exclusively with text data (Wikipedia). bGPTmix: Trained with a mix of all the above datasets. Each model was fine-tuned for specific types of classification and generative tasks post-pretraining, providing a direct comparison of each model\u0026rsquo;s performance.\nResult and Analysis\nText Processing: bGPTwiki showed high classification accuracy on the AG News dataset, indicating bGPT\u0026rsquo;s strong performance in text-based tasks. Audio Processing: bGPTlibri demonstrated excellent performance on the Speech Commands v2 dataset, showcasing its high potential in audio processing. Image Processing: bGPTimage recorded high accuracy on the CIFAR-10 dataset but showed somewhat lower performance on ImageNet. This suggests that while bGPT works well with relatively simple images, it may have limitations with more complex images. Model | AG News (4 classes) | CIFAR-10 (10 classes) | Speech Commands v2 (36 classes) | BPB Acc (%) BPB Acc (%) BPB \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; \u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; bGPT_random 1.3496 84.74 3.4928 76.73 1.5414 bGPT_wiki 1.0639 92.49 3.6663 77.02 1.5719 bGPT_image 1.4179 83.16 3.1234 88.69 1.5326 bGPT_libri 1.3993 83.59 3.3345 83.51 1.4818 bGPT_signal 1.4058 83.80 3.1554 87.65 1.4898 bGPT_mix 1.0935 91.75 3.2279 84.32 1.5086 Baselines 0.9237 94.50 ‚Äî 98.13 ‚Äî 2. Algorithm and Hardware Simulation\nExperiment Overview One of the unique capabilities of the bGPT model is its ability to simulate the operations of algorithms and hardware. This experimental section assesses how bGPT handles complex data conversion processes and CPU state modeling tasks. These capabilities are particularly significant in the fields of cybersecurity, system diagnostics, and hardware optimization.\nExperiment Methods bGPT\u0026rsquo;s performance was evaluated in the following two key areas:\nData Conversion: This experiment evaluates whether bGPT can learn the process of converting ABC music notation into MIDI format. The task tests how bGPT models complex algorithms and their ability to convert actual music files. CPU State Modeling: CPU state modeling assesses how bGPT predicts and updates the state of a CPU based on a given set of machine instructions. This is particularly useful for understanding and predicting hardware operations. Results and Analysis\nData Conversion Performance: bGPT performed the conversion between MIDI and ABC notation with high accuracy. Notably, it also showed high accuracy in converting MIDI back to ABC notation, indicating that bGPT successfully learned the inherent structures and patterns of the data. CPU State Modeling Performance: bGPT accurately predicted the resulting state of CPUs from an initial state across a variety of CPU instructions. It achieved over 99% accuracy even with complex instruction sequences, demonstrating bGPT\u0026rsquo;s detailed understanding of the internal workings of hardware. Conclusion and Future Work # bGPT has proven to be a powerful model capable of effectively processing various types of digital media data. Particularly, this model can be flexibly applied to different types of data and has demonstrated performance that can compete with models pretrained on specific datasets. These results show that bGPT can be extremely useful in solving a wide range of real-world problems.\nMoreover, bGPT has proven its ability to go beyond simply processing data, successfully modeling and simulating the operations of complex algorithms and hardware. This capability will be particularly valuable in fields related to technical problem solving and new hardware design.\nbGPT extends deep learning to binary data processing through byte prediction. Experiments have demonstrated bGPT\u0026rsquo;s strong scalability in native binary data modeling.\nFuture research directions for byte models include:\nReducing training costs to make byte model training more feasible. Expanding the model and dataset sizes to accommodate a wider range of native binary data and handle larger digital media files such as high-resolution images and videos. Improving performance in underexplored tasks involving native binary data across various application domains. "},{"id":23,"href":"/docs/spring24/24_/","title":"24","section":"Spring24","content":" # "},{"id":24,"href":"/docs/spring24/25_/","title":"25","section":"Spring24","content":" Merging Text Transformer Models from Different Initializations # Posted by: Kyungtae Kim, Minwoo Kim\nAuthors: Neha Verma (Johns Hopkins University), Maha Elbayad (Meta)\nAlthough recent works on model merging have exhibited low- or zero-barrier mode connectivity between models with different initialization, model merging on transformer architecture has not yet been studied extensively. The application of previous merging techniques on the transformer structure is limited due to its unique structural characteristics, such as residual connection, multi-head attention (MHA), and sequential input. The paper merges separate transformer minima, proposing a new model merging technique to investigate the relationship between the pre-trained models\u0026rsquo; minima in the loss landscape. Using permutation-based model merging, authors found lower loss barriers between minima compared to other model merging techniques such as model averaging. The results showed that the model has less sharp and isolated minima than previously expected.\nThe contributions of the researchers are listed as follows:\nThey introduced a new transformer merging algorithm based on model permutation. They showed that the technique leads to decreased loss barriers between masked language models trained from different initializations compared to other merging methods. They extended their approach to fine-tuned models and showed consistently smaller loss barriers between models compared to vanilla merging. Background # Transformer # Transformer is a type of sequence-to-sequence (seq2seq) models that takes a sequence of tokens as an input, and computes according to the input token. Unlike previous seq2seq models where a certain input token had a hard time affecting every output tokens, transformer uses self-attention, which allows all tokens to affect every output tokens. This allows for better performance in data where the distance between tokens has low relationship to the importance of the tokens\u0026rsquo; importance. For more details on transformers and attention, see the paper \u0026lsquo;Attention is All You Need\u0026rsquo;.\nLoss Landscape # Loss landscape is a representation of the loss values around the weight space of the network. Loss landscape helps researchers see how well a neural network has been trained and gives researchers new insights on their models.\nDNNs are trained by optimizing a loss function with an stochastic gradient descent (SGD) variant. The loss landscapes of these networks have been shown to contain infinitely many global minimizers that are reachable with the help of SGD. One reason for the abundance of minima is overparameterization, which leads different functions to behave similarly on the training data. Permutation and scaling invariances also lead to functionally identical minima that differ in the weight space. Prior works stated that the optima of loss functions are connected by simple curves over which training and test accuracy are nearly constant (no loss barrier). This is called mode connectivity. Other researchers conjectured that if the permutation invariances of neural networks are taken into account, these optima are linearly mode connected, i.e. the linear path connecting these two models has no loss barrier. In the paper, the authors pay attention on how permutation between models could lead to similar or identical loss landscapes.\nModel Interpolation # Model interpolation is a technique that blends two or more models to create an intermediate model. This process is mostly done by averaging the model weights. Researchers found out that if fine-tuned models lie in a single low error basin, then the weight averaging performs similarly to ensembling, which combines the output of multiple fine-tuned model to hopefully obtain a better result. It is however not guaranteed that fine-tuned models (starting from the same initialization) will reside in the same loss basin. Prior work on linear interpolation-based model merging has focused on improving the algorithms used to bring the hidden units of two networks into alignment, in order to reduce the barrier to interpolation between them.\nPermutation-based Merging # Feed-Forward Layers # In this section, we explain how the authors of the paper used permutation to find the similarities between two distinct models and merge them. Given two models \\(\\theta_A\\) and \\(\\theta_B\\) trained from distinct initializations, the authors compute post-activation features for each layer or sublayer parameter \\(\\text{W}_l\\subset \\theta\\) in order to compute the similar parts across models. The researchers compute \\(d\\) -dimensional activations across \\(n\\) tokens from both models \\(\\text{X}_A, \\text{X}_B\\in \\mathbb{R}^{n\\times d}\\) . Then, the feature relatedness via cross-correlation is computed as\n\\[C=\\text{corr}(\\text{X}_A, \\text{X}_B)=\\frac{\\mathbb{E}[(\\text{X}_A-\\boldsymbol{\\mu}_{\\text{X}_A})^\\text{T}(\\text{X}_B-\\boldsymbol{\\mu}_{\\text{X}_B})]}{\\boldsymbol{\\sigma}_{\\text{X}_A}\\boldsymbol{\\sigma}_{\\text{X}_B}},\\] where \\(\\boldsymbol{\\sigma}\\) is a standard deviation vector, and \\(\\boldsymbol{\\mu}\\) is a mean vector. The features are standardized since the magnitude of features values can vary greatly depending on the initialization. Next, the permutation that gives the highest correlation score is computed, and is declared as the optimal computation. More specifically, given \\(C\\in\\mathbb{R}^{d\\times d}\\) and a permutation mapping \\(\\pi\\) , the optimal permutation is computed as follows:\n\\[\\text{arg}\\max_\\pi \\sum_{i=1}^{d} C(i, \\pi(i)).\\] cf. The above problem is solved using the Jonker-Volgenant algorithm.\nNext, the permutation mapping \\(\\pi\\) is converted to a permutation matrix \\(\\text{P}\\) . The matrix is then multiplied to the original weight matrix of \\(B\\) denoted as \\(\\text{W}_l^B \\subset \\theta_B\\) . Then the permuted weight matrix \\(\\text{P}\\text{W}_l^B\\) closely resembles the weight \\(A\\) , denoted as \\(\\text{W}_l^A \\subset \\theta_A\\) . Denoting the modified model parameters as \\(\\theta_B\u0026#39;\\) , the final merged model is computed as \\(\\lambda\\theta_A\u0026#43;(1-\\lambda)\\theta_B\\) for some \\(\\lambda\\in[0,1]\\) . cf. If permutation matrix \\(\\text{P}\\) is multiplied in layer \\(l\\) , then \\(\\text{P}^{\\text{T}}=\\text{P}^{-1}\\) is applied in the next layer to unpermute the ordering, i.e.,\n\\[\\text{W}_{l\u0026#43;1}^{B\u0026#39;} \\leftarrow \\text{W}_{l\u0026#43;1}^{B}\\text{P}^{\\text{T}}.\\] Multi-head Attentions # Multi-head attention parameters include parameters from key, query, value, and linear layer each denoted as \\(\\text{W}_K\\) , \\(\\text{W}_Q\\) , \\(\\text{W}_V\\) , and \\(\\text{W}_O\\) . For each key, query, and value weights, the whole parameter \\(\\text{W} \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{model}}}\\) is partitioned into \\(H\\) attention heads each of output dimension \\(d_k = d_{\\text{model}}/H\\) . Permutation should be operated on each attention head separately, in order to apply a permutation to full weight matrices and maintain the functional equivalence of the overall model. This is because the final hidden vector from MHA reflects a concatenation of the result from each head, which are computed separately with weights \\(\\text{W}_{K_i} ,\\text{W}_{Q_i} , \\text{W}_{V_i}\\) for head \\(i\\) . In the paper\u0026rsquo;s case, since the models are trained from different initializations, the correspondence of their attention heads may differ in addition to the correspondence of features within each head. The features are extracted just after the attention computation and before the linear layer. The features are used to compute \\(C\\) , and then the correlation matrix is partitioned by heads into \\(d_k \\times d_k\\) correlation matrices, for each potential attention head pair. Next, optimal permutation for each unique head pair \\((j, k)\\) is computed. Each head\u0026rsquo;s internal permutation is computed and stored, and the cost is computed as\n\\[\\text{cost}(j,k)=\\max_\\pi \\sum_{i=1}^{d_k} C_{jk}(i,\\pi(i)),\\] where \\(C_{jk}\\) refers to the specific partition of the overall correlation matrix. The outer head correspondence permutation is computed as\n\\[\\pi_{\\text{outer}}=\\text{arg}\\max_\\pi \\sum_{h=1}^{H} \\text{cost}(h,\\pi(h)).\\] The algorithm returns a permuting matrix \\(\\text{P}_{\\text{MHA}}\\) , which is applied to each of \\(\\text{W}_V\\) , \\(\\text{W}_K\\) and \\(\\text{W}_Q\\) .\nResidual Connections # Each transformer layer comes with two residual connections, as can be seen from FIgure 1. The residual connections can be formulated as follows:\n\\[\\begin{align} x_a^r\u0026amp;=\\text{LN}(\\text{W}_O \\text{MHA}(x) \u0026#43; x),\\\\ x_f^r\u0026amp;=\\text{LN}(\\text{W}_2 \\text{ReLU}(\\text{W}_1 x_a^r) \u0026#43; x_a^r). \\end{align}\\] The input and output of both sublayers are added to create a new output. This implies that if a permutation operation is applied to the output state, the permutation should be the same for both addends. Also, since the inputs passes through the LayerNorm module, the permutation to the output should also permute the features of the LayerNorm module also. Ignoring the parameters of the LayerNorm,\n\\[\\begin{align} \\text{P}x_f^r\u0026amp;=\\text{P}(\\text{W}_2 \\text{ReLU}(\\text{W}_1 x_a^r) \u0026#43; x_a^r)\\\\ \u0026amp;=\\text{P}\\text{W}_2 \\text{ReLU}(\\text{W}_1 x_a^r) \u0026#43; \\text{P}x_a^r\\\\ \u0026amp;=\\text{P}\\text{W}_2 \\text{ReLU}(\\text{W}_1 x_a^r) \u0026#43; \\text{P}(\\text{W}_O \\text{MHA}(x) \u0026#43; x) \\end{align}\\] Since the input to each layer must be permuted ( \\(\\text{P}x\\) ), and the output of each layer is also permuted ( \\(\\text{P}x_f^r\\) ), the entire transformer architecture uses the same \\(\\{\\text{P}, \\text{P}^{\\text{T}}\\}\\) matrices for all weights involved in residual connections.\nModels, Tasks, Datasets and Evaluation settings # In this work, the authors investigated 5 different BERT models from the MultiBERTs reproductions seeds 1 through 5 (See \u0026lsquo;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u0026rsquo; \u0026amp; \u0026lsquo;The MultiBERTs: BERT Reproductions for Robustness Analysis\u0026rsquo;). Each model has the following properties:\nBert-base-uncased checkpoint. Different random initialization and random ordering. Same original BERT vocabulary and tokenizer. To test the baseline method, they used the masked language modeling task while employing the validation set of the Wikitext-103 benchmark as the evaluation data. Next, they extracted over one million sentences from the Books corpus. In classification tasks, they employed fine-tuned models with randomly initialized classification head with pooling layer and classification layer weights. The authors kept the head initializations the same across the models. They used the General Language Understanding Evaluation (GLUE) benchmark excluding WNLI. As a baseline for comparison, vanilla averaging is defined as:\n\\[\\theta_{avg} = \\frac{1}{2}(\\theta_A\u0026#43;\\theta_B)\\] In this work, they defined new evaluation definitions. They defined loss-barriers as (\u0026lsquo;M. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In International Conference on Learning Representations\u0026rsquo;): \\[\\max_{\\lambda} \\mathcal{L}(\\lambda\\theta_A \u0026#43; (1 - \\lambda)\\theta_B) - \\frac{1}{2}(\\mathcal{L}(\\theta_A) \u0026#43; \\mathcal{L}(\\theta_B))\\] A masking probability of \\(p = 0.15\\) across block sizes of 128 tokens were used to compute MLM loss/pseudo-perplexity. For \\(N\\) masked samples in the text \\(\\textbf{W}\\) , pseudo-perplexity is defined as:\n\\[\\mathrm{Pseudo-PPL}(\\textbf{W};\\theta) = 2^{-\\frac{1}{N} \\sum_{i=1}^{N}\\log_{2}\\,p_\\theta(\\omega_i|\\textbf{W}_{\\backslash{i}})}\\] Results # By component # First, they found that the merging all feed-forward sublayers and/or merging all multi-headed attention sublayers reduces the pseudo-perplexity compared to the baseline. Remakably, combination of them leads to the reducuction of the perplexity by about 7 times at \\(\\lambda = 0.5\\) (See Figure 3). The reduced barrier suggests that a lower loss path has formed among these models, indicating a connection between the minima with a barrier similar to what they report.\nFigure 3. Results of pseudo-perplexity scores of 10 MultiBERTs with vanilla averaging, merging all feed-forward sublayers, and merging all multi-headed attention sublayers and all multi-headed attention sublayers. Next, they investigated how well these Transformers learn similar representations of the model. The average feature correlations of both the Feed-Forward layer and the attention pre-merged/merged with our method are calculated. The aligned models show higher average feature correlations than the orignal models. However, these values are no more than 0.3 because some pre-trained transformers can be sparsely activated and be pruned heavily leading to lower average feature correlations (Li et al.,2023; Dalvi et al., 2020).\nFigure 4. Results of average feature correlations between 10 masked language model pairs. Multi-headed attention # Then, they investigated loss barrier of Head-Permutation multi-headed attention approach. It is worth noting that this approach maintains head structure while allowing different head correspondences (Head-Perm). The proposed method exhibits lower loss barrier than method using simple attention averaging (Vanilla Attention Avg.), method that ignores the multiheaded structure of the weight parameters (Ignore-Heads), and method that does not allow for different head correspondences across different models (Monotonic) while exhibiting clear attention head boundaries of the correlation matrix (See Figure 5 and Table 1).\nMethod Loss Barrier\u0026darr; Std. Err. Vanilla Attention Avg. 4.31 0.21 Monotonic Head Alignment 4.13 0.20 Ignore-Heads 3.97 0.25 Head-Perm 3.71 0.23 Table 1. Loss Barriers of 10 MultiBERTs merged with feed-forward and attention components merged. Figure 5. Results of a correlation matrix between the first multi-headed attention layer from two different MultiBERTs models. Residual Stream # Moreover, they investigate the effect of the permutation alignment involving residual connection parameters. As described in Residual Connections, repeated Add/Norm components sharing the permutation operations reduce the permutation symmetries and available residual stream parameters. The identity permutation which uses the identity matrix \\({I_d}\\) exhibits the lowest loss barrier because only one pair of \\({\\{P, P^T\\}}\\) is in the residual stream. We note that the seperate permutation approach, despite it having the largest loss barrier and no valid symmetry, has largest degrees of freedom.\nMethod Loss Barrier\u0026darr; Std. Err. Identity 4.95 0.38 First 7.58 0.19 Last 7.41 0.18 All 7.34 0.22 Seperate 9.38 0.49 Table 2. Loss Barriers of merged MultiBERTs with only residual components merged. Amount of Data # Moreover, they investigate the effect of the amount of sentences on the loss barrier. Despite combination of feed-forward and attention layers, there is no strong directional relationship between the amount of data and the loss barrier. It seems that some variations are attributed by the quality of the data (See Figure 6).\nFigure 6. Results of loss barrier respect to the amount of sentences. GLUE results # Finally, they compared the loss barriers of their method to those of vanilla averaging approach for eight different GLUE tasks including residual permutations (See Figure 7 and Table 3). Vanilla averaging (STS-B) exhibits the highest loss, but some tasks show that the vanilla averaging outperforms their approach. They observe inconsistent loss reduction, with lower loss barriers than those of the masked language modeling setting. They also observe that lower loss pattern than either parent model at about \\(\\lambda = 0.15\\) and \\(\\lambda = 0.85\\) . Interestingly, M-shaped curve can be found in some vanilla merges between these fine-tuned models. In this perspective, their method could be extended to explore lower loss paths between finely-tuned minima. However, the selection of optimal data for the lowest loss, understanding of fine-tuned models and pre-connectivity in the loss landscape are remained for future work.\nFigure 7. Loss barrier curves for 8 GLUE tasks for vanilla interpolation and our strategy. Vanilla averaging Proposed Barrier Error Barrier Error MNLI-mm 0.61 0.03 0.72 0.08 QQP 1.37 0.09 1.20 0.11 QNLI 0.64 0.04 0.77 0.06 SST-2 0.42 0.04 0.36 0.07 CoLA 1.31 0.14 1.11 0.13 STS-B 5.15 0.44 4.24 0.35 MRPC 2.74 0.08 1.93 0.11 RTE 0.53 0.04 0.41 0.05 Table 3. Comparison of loss bariers between fine-tuned BERT model across 8 GLUE tasks for vanilla interpolation and our strategy. Conclusion # In this work, the authors develop a new strategy for model mergring based on permutation mapping and demonstrates reduced loss barriers between masked languaged models with different initialziation compared to vanilla merging. Next, they extend their approach to fine-tuned models. The authors suggest that understanding the connectedness between models lead to achieving sharpness of minima and smoothness Transformer loss space. Moreover, it can open up new possibilities for improving design optimization methods, ensembles of models, and additional merging techniques. Specifically, this paper shows that permutation invariances of Transformer model is considered to characterize the geometric features of minima. Finally, they shad the light on the relationships between fine-tuned models, Transformer width and loss barriers, and the data for characterize the relationship between Transformer minima.\nReferences # https://arxiv.org/abs/2403.00986\nLoss landscape figure\n"},{"id":25,"href":"/docs/spring24/17_/","title":"17","section":"Spring24","content":" QuaRot : Outlier-Free 4-Bit Inference in Rotated LLMs # Author : Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L. Croci, Bo Li\nPosted by MyeongJi Yun, JungGyu Min, POSTECH\nThis post assumes that the reader has a structural understanding of Transformer and Llama models. If you need a detailed understanding of these models, please refer to the Transformer, LLaMa.\nLarge Language models ( LLMs ) like GPT-2, LLaMa have become increasingly important due to their countless applications. However, their inference requires a significant amount of computation, memory, and energy. Quantization is among the most important techniques to solve both memory and compute issues in LLM inference.\nOutlier makes quantization difficult # Recent research has shown that LLMs have large outliers and make quantization more difficult, especially in 4-bit case. Also, they mentioned that the activations have more outliers, which makes quantization harder. There are three main streams to solve this problem.\nWeight only quantization LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale, 2022 NeurIPs Weight quantization can ease the memory budget for saving the model. However, since activations are not quantized, the computation still involves integer and float operations, making it difficult to address compute issues. Remain outlier in higher bitwidth QUIK: Towards End-to-End 4-Bit Inference on Generative Large Language Models, 2023 Weight quantization can ease the memory budget for saving the model, and since most operations are integer X integer, compute issues are largely resolved. However, some operations still involve integer X float, and the occurrence of float values is irregular, leaving some compute issues unresolved. Use calibration set and normalize activation SmoothQuant: Accurate and Efficient Post-Training Quantization for LLM, 2023 ICML Accuracy is guaranteed up to 8-bit quantization, but it is not assured with 4-bit quantization. In ‚Äú QuaRot : Outlier-Free 4-Bit Inference in Rotated LLMs‚Äù, the author introduces a new method for quantizing LLM models end-to-end, by utilizing ‚Äúcomputational invariance‚Äù to all weights and activation and optimizing the computing process.\nRandom Hadamard transform doesn‚Äôt change the result # In the concept of computational invariance theorem, small changes in input parameters do not cause the output difference if the algorithm is stable. When applying this to a transformer-based large language model (LLM), it implies that rotating the coordinate system of activations between weight and computation blocks using an orthogonal matrix does not alter the model\u0026rsquo;s output. According to this theory, instead of using any matrix X that constitutes the transformer, you can use X‚Ä≤=UXV where U and V are orthogonal matrices, and the computational results will remain unchanged.\nIf the number or proportion of outliers in ùëã‚Ä≤ is less than that in ùëã, the information loss during quantization can be reduced. The paper \u0026ldquo;A\u0026rdquo; demonstrates that multiplying a matrix by orthogonal matrices on both sides reduces the value of max‚Å°(ùëã)/mean(ùëã). This means that the presence of extreme values relative to the average is diminished, leading to a more uniform distribution of values within the matrix. However, performing ùëàùëãùëâ also incurs overhead, so selecting orthogonal matrices ùëà and ùëâ that minimize this overhead is essential.\nQuaRot uses Random Hadamard transformation because the result PPL is lower, so random Hadamard transformation is better than random matrix.\nLLama2-7B LLama2-7B LLama2-7B QuaRot ( Random ) 7.45 5.84 4.07 QuaRot (Hadamard) 6.10 5.40 3.79 Random Hadamard transformation matrix H is described below :\n$$ H_{2} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \u0026amp; 1 \\ 1 \u0026amp; -1 \\end{bmatrix}, \\quad H_{2^n} = H_2 \\otimes H_{2^{n-1}} $$\n$$ H\u0026rsquo; = H \\cdot \\mathrm{diag}(s), \\quad s \\sim \\mathrm{Uniform}({-1, +1}) $$\nThis transformation pairs elements to perform simultaneous computations, allowing the matrix-vector multiplication between matrix ùêª and vector ùë• to be executed using only ùëÇ(ùëëlog‚Å°ùëë) addition operations without any multiplications, as illustrated below:\n--- QuaRot demonstrates that using this technique reduces the number of outliers. By applying the random Hadamard transformation, the distribution of activations is more uniform, which decreases the number of extreme values or outliers, thereby minimizing information loss during quantization.\nStep by Step modification and quantization # Step 1 involves applying the new schemes proposed by QuaRot to significantly reduce outliers in weights and activations, thereby minimizing accuracy degradation due to quantization held in Step 2. The key technique is to apply the Hadamard transform to each activation and weight in both attention blocks and FFN. This is done by merging operations through the use of two different Hadamard transform matrices across consecutive layers, creating an optimal computational flow.\nStep 1-a. Weight Modification # Note that the multiplication of two orthogonal matrices generates identical matrix, so inserting Q and Q^T between linear layers doesn‚Äôt change any output.\n$$ I = Q Q^T, XQQ^TW = XW $$\nConsidering LayerNorm or RMSNorm at the start of the transformer multiplying some orthogonal matrices does not change output. Also, we can fuse the scaling operation of RMSNorm‚Äôs : diag(a) into an adjacent weight matrix.\n$$ RMSNorm(X) = x_i \\leftarrow \\frac{x_i}{||x_i||} = ( \\frac{x_i * Q}{||x_i||} ) Q^T = RMSNorm (XQ^T)Q $$\nSo for all weights after the RMSNorm layer, the weight becomes :\n$$ W \\leftarrow Q^T diag(a) W, Q = Hdiag(s) $$\nStep 1-b. Rotate FFN # Inserting online Hadamard operation can ease the activation value‚Äôs quantization difficulty within each block. This operation is implicitly reserved by fusing a Hadamard matrix into the next matrix of the network.\nStep 1-c. Attention Value Projection # This step applies Hadamard transformations to the value and output projection matrices in the attention block throughout both offline weight modification and online activation transformation. Since value and output projection weight are multiplied in each head, two matrices can be transformed using the Hadamard matrix without changing the result of attention.\n$$ W_v^{(h)} \\leftarrow W_v^{(h)}H_{d_h}\\W_{out}^{(h)} \\leftarrow H_{d_h} W_{out}^{(h)}\n$$\nThis transformation can be represented with Kronecker multiplication in the point of full attention computation view.\n$$ W_v \\leftarrow W_v(I\\otimes H_{d_h})\\W_{out}\\leftarrow (I\\otimes H_{d_h}) W_{out}\n$$\nThe following simple lemma defines the remaining Hadamard operation after modification.\n$$ H_{a\\times b}= (I\\otimes H_{b}) (H_{a}\\otimes I ) $$\nThis defines the remaining Hadamard operation as the later term of the upper lemma, which results in a modification of the online forward path.\n$$ Z \\leftarrow Z(H_{n_h} \\otimes I) $$\nStep 1-d. Key Rotation # This step applies Hadamard transformation to the key vectors in the attention module. Utilizing the RoPE method (Su et al., 2021), the positional encoding is directly attended to query and key vectors. This reshapes the attention score computation equation into a modification-convenient form.\n$$ \\text{Score}=\\text{Softmax}(\\alpha \\text{Pos}(Q_h) \\text{Pos}(K_h^T)\\odot M) $$\nThe Hadamard transformation is applied to both position encoded query and key vectors similar to step 1-c.\n$$ \\text{Pos}(Q) = \\text{Pos}(XW_q) \\leftarrow \\text{Pos}(XW_q)(I\\otimes H_{d_h})\\\\text{Pos}(K) = \\text{Pos}(XW_k) \\leftarrow \\text{Pos}(XW_k)(I\\otimes H_{d_h})\n$$\nNote that this transformation can be applied without changing final attention scores since both queries and keys are rotated, therefore no remaining Hadamard transformation exists.\nStep 2 involves applying various state-of-the-art techniques to quantize weights and activations.\nStep 2-a. Weight Quantization # You can quantize the adjusted weights using GPTQ, or you can use a very simple round-to-nearest (RTN) technique. The paper have shown simpler method(RTN) have shown a slight sacrifice in accuracy.\nStep 2-b. Online Quantization # To quantize the activations, find the scale factor for each row (max(row) / 7), then divide all values by the scale factor and convert them to the nearest 4-bit integer. For dequantization, multiply the 32-bit integer output of GEMM by the scale factors of both the activation and the weight, and convert the result to FP16.\nStep 2-c. Quantized Attention # The significance of storing in 4-bit is greater than performing calculations in 4-bit because attention operations are memory-bound. Thus, to compute attention, keep the query, key, and value in FP16 and use Flash Attention for the softmax computation.\nQuaRot saves runtime \u0026amp; memory # As highlighted in the contributions of the paper, this model demonstrates that it maintains accuracy even with 4-bit quantization, achieving the same level of accuracy as other models with significant computation overhead.\nThe key point of QuaRot is that the process of performing the Hadamard transform for quantization to INT4 should not introduce a large overhead compared to the computational benefits gained from converting to INT4. From the perspective of the runtime of the FFN block, it has been confirmed that the overhead remains minimal regardless of layer size, model size, or batch size. Additionally, the memory saving factor ranges from x3.48 to x3.71, which is very close to the ideal value (4 = FP16 / INT4), demonstrating significant efficiency. This paper is particularly noteworthy for addressing the issue of memory overhead in long sequence scenarios by quantizing the KV cache as well.\nDiscussion and future work direction # Why we limited to symmetric INT4 qunatization? Numerous papers discuss the limitations of using symmetric quantization in INT4 format for quantization. For example, ANT demonstrate that, even with the same bitwidth, numeric formats like flint and PoT(power of Two), which divide the representation into exponent and mantissa, can achieve better accuracy due to their ability to represent a wider range of values. In the figure below, the INT-4bit example uses only integers, while the others utilize new data formats. It is evident that the Mean Squared Error (MSE) significantly decreases with these new formats.\nQuaRot considers INT4 format for both weight quantization and activation quantization, likely because modern GPUs support efficient operations with INT4 and INT8 formats. If we could use other formats, it might be possible to maintain accuracy even with formats as small as 3-bit, leading to greater memory savings. However, maintaining computational simplicity is challenging because GPUs are not optimized for operations with custom data types, unlike INT4. Therefore, achieving optimal computation with custom data types would require the development of custom hardware.\nQuantization + Pruning One of the authors, Dan Alistarh, has papers on GPTQ and OBS. GPTQ focuses on reconstructing matrices after quantization, while OBS deals with reconstructing models after pruning. Both papers share a common foundation in using the Hessian matrix and employ various optimization techniques such as Wood-Fisher. Combining these two approaches, the OBC study explores methods to preserve the accuracy of networks that undergo both pruning and quantization. Another paper involving the author demonstrates that SliceGPT similarly achieves effective pruning by employing the concept of computational invariance when multiplying orthogonal matrices. By analyzing the properties of orthogonal matrices in both QuaRot and SliceGPT, I believe it is possible to achieve quantization and pruning simultaneously. How to reduce the overhead of online Hadamard transformation The forward path in QuaRot mostly follows the activation-quantized LLMs like (), yet requires the additional task of online Hadamard transformation on attention activation. Similar to non-matmul tasks, the online Hadamard transformation can be performed by existing computational resources by converting the task into a matrix-multiplication form or tossing a task to a dedicated hardware accelerator. Unlike existing non-linear operations in conventional LLMs, the Hadamard transformation "}]